# Machine Learning Lab Architecture ‚Äì FastAPI + Scikit-learn + XGBoost

Este proyecto explica paso a paso c√≥mo funciona el algoritmo **XGBoost**
mediante una PoC construida con:

- Scikit-learn (ecosistema / pipelines / m√©tricas)
- XGBoost (motor de Machine Learning)
- FastAPI (capa de exposici√≥n REST)
- Exception Handling (mecanismo de gesti√≥n y control de errores)

---
## üß± Requisitos

- Python 3.9+
- xgboost
- scikit-learn
- pandas
- numpy
- joblib
- fastapi
- uvicorn

---
## üéØ Objetivo

Explorar c√≥mo el algoritmo XGBoost puede ser integrado en una arquitectura moderna de Machine Learning, cubriendo:

1) **Fundamentos te√≥ricos de XGBoost**
2) **Integraci√≥n de XGBoost mediante la API de Scikit-learn**
3) **PoC ‚Äì Arquitectura y Implementaci√≥n de un Pipeline de Machine Learning con XGBoost y Scikit-Learn**
4) **Decisiones T√©cnica de Arquitectura** 
---

## 1. Fundamentos te√≥ricos de XGBoost
XGBoost (eXtreme Gradient Boosting) es una librer√≠a de machine learning que implementa el algoritmo de Gradient Boosting sobre √°rboles de decisi√≥n, combinando m√∫ltiples modelos d√©biles para construir modelos predictivos altamente eficientes y precisos, optimizada para ser:

- ‚ö° Eficaz
- üéØ Precisa
- üß± Robusta frente al overfitting
- üìä Excelente para datos tabulares

> üß†  **Gradient Boosting** es un algoritmo de aprendizaje supervisado que aprende relaciones y estructuras en los datos mediante la combinaci√≥n secuencial de modelos d√©biles (normalmente √°rboles de decisi√≥n). <BR>
>> Funci√≥n: **f(X)‚ÜíY**

### Ciclo de entrenamiento iterativo de XGBoost

XGBoost entrena m√∫ltiples √°rboles de decisi√≥n poco profundos de manera secuencial, donde cada nuevo √°rbol se ajusta a los errores (residuos) del conjunto de √°rboles anteriores, y la combinaci√≥n ponderada de todos ellos da lugar al modelo final de Gradient Boosting.

**> Ciclo Base::**

‚úÖ Se entrena un √°rbol muy simple.<br>
‚úÖ Ese √°rbol comete errores, que se calculan expl√≠citamente.<br>
‚úÖ El siguiente √°rbol se centra en corregir esos errores <br>
‚Üí aprende a predecir el residuo (error), no el valor final. <br>
‚úÖ El proceso se repite iterativamente muchas veces. <br>
‚úÖ Al final, se combinan todos los √°rboles optimizados para formar un modelo de Gradient Boosting <br>


### Componentes estructurales de XGBoost

Componentes principales que compone la libreria XGBoost:

**1) Matrix ‚Üí estructura optimizada de datos**<br>
**2) Booster ‚Üí motor interno del modelo**<br>
**3) train() ‚Üí API nativa de XGBoost**<br>

> üß†  **NOTA:** En esta PoC nos apoyaremos exclusivamente en el componente Booster, responsable del mecanismo interno de aprendizaje del modelo. El acceso y la operaci√≥n del modelo de XGBoost se efectuar√°n a trav√©s de la API compatible con Scikit-learn, facilitando su integraci√≥n en pipelines de ML

#### API Nativa de XGBoost

**1.M√©todos de entrenamiento (Training API)** <br>
Son los que construyen el modelo:
* fit() ‚Üí Entrena el modelo
* set_params() ‚Üí Ajusta hiperpar√°metros
* get_params() ‚Üí Recupera configuraci√≥n

**2. M√©todos de inferencia (Inference / Prediction API)** <br>
Son los que usa tu aplicaci√≥n en runtime:
* predict() ‚Üí Clase / valor predicho
* predict_proba() ‚Üí Probabilidades (clasificaci√≥n)

**3. M√©todos de evaluaci√≥n (Evaluation API)** <br>
Relacionados con m√©tricas:

* score() (wrapper sklearn)
* evals_result()
* m√©tricas configuradas (eval_metric)

**4. M√©todos de persistencia (Model Persistence API)**<br>
Gestionado el ciclo de persistencia de los modelos:

* save_model()
* load_model()
* get_booster()
---
# 2. Integraci√≥n de XGBoost mediante la API de Scikit-learn

**XGBoost wrappers ‚Üí sklearn API contract:**

B√°sicamente, scikit-learn act√∫a como framework de orquestaci√≥n (pipelines, preprocesado, m√©tricas), mientras que XGBoost funciona como el motor de Machine Learning responsable del entrenamiento y la optimizaci√≥n del modelo mediante **Gradient Boosted Decision Trees**.

| Machine Learning   | Funcion      | Significado pr√°ctico                 |
|--------------------|--------------|--------------------------------------|
| **`scikit-learn`** | Framework    | framework de orquestaci√≥n            |
| **`XGBoost`**      | Motor Modelo | motor de aprendizaje especializado   |

La integraci√≥n entre **Scikit-learn y XGBoost** se produce gracias a los wrappers proporcionados por XGBoost (XGBClassifier, XGBRegressor), los cuales implementan la interfaz est√°ndar de estimadores de **sklearn**.
Este enfoque permite que **Scikit-learn** act√∫e como capa de orquestaci√≥n del pipeline, mientras que **XGBoost** opera como motor de aprendizaje subyacente.

**Flujo de Invocacion Interna:**

![Texto alternativo](/dev-xgboost-learning-py/resources/docs/img/flow_xgboost_scikitlearn.jpg)

1Ô∏è‚É£ **Scikit-learn Pipeline ::** <br> 
Esta la capa de orquestaci√≥n<br>

2Ô∏è‚É£ **Estimator Interface (fit / predict) ::**<br>
Esta capa representa el contrato est√°ndar de sklearn. Scikit-learn no sabe si el modelo es 
* RandomForest
* LogisticRegression
* XGBoost
* LightGBM

3Ô∏è‚É£ **XGBClassifier Wrapper ::**<br> 
Aqu√≠ ocurre la magia de integraci√≥n.
El wrapper: <br> 
* Implementa la interfaz sklearn <br> 
* Traduce llamadas ‚Üí motor XGBoost <br> 
* Convierte datos ‚Üí DMatrix <br> 

4Ô∏è‚É£ **XGBoost Booster (Core Engine) ::** <br> 
Este es el motor real de Machine Learning.

Aqu√≠ sucede:

* Gradient Boosting <br> 
* Construcci√≥n de √°rboles <br> 
* Optimizaci√≥n <br> 
* Regularizaci√≥n <br> 

El Booster no conoce sklearn. Solo ejecuta aprendizaje.

### ‚öôÔ∏è Scikit-Learn ::  Componentes y M√©todos de Machine Learning


| M√©todo                         | Firma / API | Tipo | Capa Arquitect√≥nica | Rol en el Flujo |
|--------------------------------|-------------|------|----------------------|------------------|
| **make_classification**        | `make_classification(n_samples, n_features, ...)` | Generador de datos | Preparaci√≥n de datos | Genera un dataset sint√©tico para experimentaci√≥n y validaci√≥n |
| **train_test_split**           | `train_test_split(X, y, test_size, random_state, ...)` | Utilidad | Preparaci√≥n de datos | Divide el dataset en subconjuntos de entrenamiento y prueba |
| **GradientBoostingClassifier** | `GradientBoostingClassifier(**hyperparameters)` | Estimador (ML Model) | Motor de aprendizaje | Implementa Gradient Boosted Decision Trees |
| **pipeline.fit**               | `pipeline.fit(X, y)` | M√©todo | Orquestaci√≥n (Pipeline) | Ejecuta el entrenamiento completo del pipeline |
| **pipeline.predict**           | `pipeline.predict(X)` | M√©todo | Orquestaci√≥n (Pipeline) | Genera predicciones de clase |
| **pipeline.predict_proba**     | `pipeline.predict_proba(X)` | M√©todo | Orquestaci√≥n (Pipeline) | Genera probabilidades por clase |
| **predict_proba**              | `model.predict_proba(X)` | M√©todo del estimador | Modelo ML | Estima la probabilidad de pertenencia a cada clase |
| **accuracy_score**             | `accuracy_score(y_true, y_pred)` | M√©trica | Evaluaci√≥n | Calcula proporci√≥n de aciertos |
| **precision_score**            | `precision_score(y_true, y_pred)` | M√©trica | Evaluaci√≥n | Eval√∫a calidad de positivos predichos |
| **recall_score**               | `recall_score(y_true, y_pred)` | M√©trica | Evaluaci√≥n | Eval√∫a detecci√≥n de positivos reales |
| **f1_score**                   | `f1_score(y_true, y_pred)` | M√©trica | Evaluaci√≥n | Balance entre precisi√≥n y recall |


---
## 3. PoC ‚Äì Arquitectura y Implementaci√≥n de un Pipeline de Machine Learning con XGBoost y Scikit-Learn 

### 3.1 üß© Contexto Funcional

Esta PoC se sit√∫a en el dominio de negocio de Seguros y tiene como finalidad construir un modelo de Machine Learning orientado a la predicci√≥n del riesgo de fraude. El modelo aprende patrones de comportamiento del cliente y estima probabilidades de riesgo basadas en variables de entrada representativas del contexto de negocio.

**La PoC cubre las principales fases operativas del modelo XGBoost:**

‚úÖ Entrenamiento ‚Üí model.fit(X, y) <br>
‚úÖ Predicci√≥n ‚Üí model.predict(X) <br>
‚úÖ Probabilidades ‚Üí model.predict_proba(X) (clasificaci√≥n) <br>
‚úÖ Persistencia del modelo ‚Üí save_model() / joblib <br>
‚úÖ Metadatos / M√©tricas ‚Üí accuracy, AUC, feature  <br>

**Prop√≥sito y Salida del Modelo:**  
‚úÖ **Modelo de Machine Learning ‚Üí**  Scoring de Riesgo / Fraude
‚úÖ **Qu√© predicci√≥n debe proporcionar‚Üí** Probabilidad de impago o fraude por cliente


### 3.2 üèóÔ∏è Arquitectura T√©cnica

La PoC sigue una arquitectura desacoplada en capas:

1Ô∏è‚É£ **API Layer (FastAPI) ::**
Responsable del contrato REST, validaci√≥n de entradas y serializaci√≥n de respuestas.

2Ô∏è‚É£ **Service Layer ::**
Orquestaci√≥n del flujo de Machine Learning (transformaci√≥n ‚Üí predicci√≥n ‚Üí respuesta).

3Ô∏è‚É£ **ML Layer (Scikit-learn + XGBoost) ::**
Entrenamiento, predicci√≥n y evaluaci√≥n del modelo.

4Ô∏è‚É£ **Persistence Layer (Joblib) ::**
Serializaci√≥n y carga del pipeline entrenado.

5Ô∏è‚É£ **Exception Handling Layer ::**
Gesti√≥n centralizada de excepciones, propagando errores de forma controlada desde la capa de ML hasta la capa de API.

#### Diagrama Arquitect√≥nico ####

![Texto alternativo](/dev-xgboost-learning-py/resources/docs/img/Arch_Model-XGBoost.png)


Este dise√±o permite:

‚úÖ Separaci√≥n de responsabilidades  
‚úÖ Reutilizaci√≥n del modelo  
‚úÖ Testabilidad  
‚úÖ Evoluci√≥n hacia producci√≥n


### 3.3 Modelo y Pipeline de Entrenamiento 

**Tipo de Entrenamiento:** Aprendizaje supervisado 

**Donde tendremos:**<br>
X ‚Üí Features (datos de entrada / variables predictoras) <br>
y ‚Üí Label (verdad hist√≥rica / variable objetivo)


**Output: Label (y)**
 - LOW = 0
 - MEDIUM = 1
 - HIGH = 2 

**Input:: Features (X)**
- edad
- ingresos_mensuales
- incidentes_previos
- ratio_deuda_ingresos
- num_productos
- canal

### 3.4 Modelo de Entrenamiento

#### üìå **Training Endpoint Specification**

| Par√°metro | Definici√≥n |
|------------|--------------|
| **Operation** | Model Training |
| **Protocol** | REST |
| **Method** | POST |
| **Resource Path** | `/api/training` |
| **Output Contract** | `TrainResponse` |
| **Successful Response** | 200 |
| **Error Response** | 404 |

#### 3.4.1 Modelo Entrada de Entrenamiento

**‚úÖ JSON Request API-Friendly ‚Äì /training**
**
```json
{
      "features": [
        {
          "edad": 24,
          "ingresos_mensuales": 1500,
          "antiguedad_meses": 8,
          "incidentes_previos": 2,
          "ratio_deuda_ingresos": 0.62,
          "num_productos": 1,
          "canal": "web"
        }
      ],
      "target": [
        {
          "risk_level": "medium"
        }
      ],
      "params": {
        "n_estimators": 200,
        "learning_rate": 0.05,
        "max_depth": 3
      }
}
```
**JSON Transformado Pydantic/Schema ‚Üí Panda/DataFrame** 

```json
    {
      "X": [
        [45, 3200.0, 60, 0, 0.25, 3, 1]
      ],
      "y": [0],
      "feature_names": [
        "edad",
        "ingresos_mensuales",
        "antiguedad_meses",
        "incidentes_previos",
        "ratio_deuda_ingresos",
        "num_productos",
        "canal"
      ],
      "params": {
        "n_estimators": 200,
        "learning_rate": 0.05,
        "max_depth": 3
      }
    }
```

**(1) Elemento Entrada Datos ‚Üí features[ ] > X**

El elemento features[] define el contrato de entrada de la API REST<br>
A partir de esta estructura, los datos se normalizan y se convierten en una matriz 2D: **X ‚Üí (n_samples √ó n_features)**

**Donde:**
* Filas ‚Üí Registros de clientes (observaciones / samples)
* Columnas ‚Üí Variables predictoras (features)
* Dimensiones ‚Üí Ejemplo: 4 √ó 6 (4 clientes, 6 features por cliente)


**(2) Elemento Objetivo ‚Üí y**<br>
Representa un vector 1D que contiene la verdad hist√≥rica (ground truth) que el modelo debe aprender a predecir a partir de **X**

**Donde: y ‚Üí [0, 1, 0, 1]**

* Cada posici√≥n de y[i] corresponde exactamente a una fila de **X[i]**
* Cada valor representa la etiqueta (label) asociada al registro 

**Ejemplo de mapping:**
- 0 ‚Üí Riesgo Bajo
- 1 ‚Üí Riesgo Alto


**(3) Elemento  feature_names: []**

Define el significado sem√°ntico de cada columna de la matriz X, estableciendo:

   - **X[]** ‚Üí Matriz num√©rica que ve el algoritmo
   - **feature_names** ‚Üí el significado humano de cada columna la matriz

**Ejemplo:**

    ```
    | Columna | Posici√≥n | Feature              |
    |---------|----------|----------------------|
    | 0       | X[i][0]  | edad                 |
    | 1       | X[i][1]  | ingresos_mensuales   |
    | 2       | X[i][2]  | antiguedad_meses     |
    | 3       | X[i][3]  | incidentes_previos   |
    | 4       | X[i][4]  | ratio_deuda_ingresos |
    | 5       | X[i][5]  | num_productos        |
    | 6       | X[i][5]  | canal                |.


**(4) Elemento  params: {}:**<br>
Representa los hiperpar√°metros del modelo, es decir, los valores de configuraci√≥n que controlan el proceso de entrenamiento.
   - **n_estimators** ‚Üí N√∫mero de √°rboles que se entrenan secuencialmente.
   - **learning_rate** ‚Üí Cu√°nto aporta cada √°rbol al modelo final.
     - Valores peque√±os ‚Üí Aprendizaje m√°s lento pero m√°s estable
     - Valores grandes ‚Üí Aprendizaje m√°s r√°pido pero m√°s inestable
   - **max_depth** ‚Üí Profundidad m√°xima de cada √°rbol.
     - √Årboles poco profundos ‚Üí modelos m√°s generales
     - √Årboles muy profundos ‚Üí memorizan datos (overfitting)

**Relaci√≥n hiperparametros:**<br>
üîπ**learning_rate** controla la velocidad de aprendizaje<BR>
üîπ**n_estimators** compensa esa velocidad mediante el n√∫mero de √°rboles.<br>

**Reglas pr√°cticas:**<br>
üîπ Si bajo **learning_rate**, subo **n_estimators**<br>
üîπ Si subo **learning_rate**, bajo **n_estimators**

#### 3.4.2 Modelo de Salida ‚Äì M√©tricas y Estado del Modelo

**‚úÖ JSON Response API-Friendly ‚Äì /training**
```json

{
  "model": {
    "id": "xgb_model_20260224_222532",
    "lifecycle": "trained"
  },
  "performance": {
    "accuracy": "70.00%",
    "precision": "83.33%",
    "recall": "70.00%",
    "f1_score": "73.00%"
  },
  "training_summary": {
    "samples_used": 50,
    "feature_dimension": 7
  },
  "features": [
    "edad",
    "ingresos_mensuales",
    "antiguedad_meses",
    "incidentes_previos",
    "ratio_deuda_ingresos",
    "num_productos",
    "canal"
  ]
}
```

El objeto de respuesta resume el estado del modelo entrenado, su desempe√±o predictivo y la configuraci√≥n estructural utilizada durante el proceso de entrenamiento.

#### **üîπ Model { }**

Contiene la informaci√≥n de identificaci√≥n y ciclo de vida del artefacto ML.

- **id ‚Üí** Identificador √∫nico del modelo persistido  
- **lifecycle ‚Üí** Estado actual del modelo dentro del flujo ML  

Permite versionado, trazabilidad y recuperaci√≥n del modelo.

#### **üîπ Performance { }**

Resume las m√©tricas de evaluaci√≥n obtenidas tras el entrenamiento.

- **accuracy ‚Üí** Proporci√≥n global de predicciones correctas  
- **precision ‚Üí** Calidad de las predicciones positivas  
- **recall ‚Üí** Capacidad del modelo para detectar eventos reales  
- **f1_score ‚Üí** Balance entre precision y recall  

Estas m√©tricas permiten validar la calidad del modelo.


#### **üîπtraining_summary{ }**

Describe las caracter√≠sticas estructurales del entrenamiento.

- **samples_used ‚Üí** N√∫mero de registros utilizados  
- **feature_dimension ‚Üí** N√∫mero de variables predictoras  

Facilita auditor√≠a y debugging del modelo.


#### **üîπ features{ }**

Lista expl√≠cita de las variables utilizadas por el pipeline ML.

Define el contrato del modelo:

‚úî Orden de entrada  
‚úî Dimensionalidad esperada  
‚úî Interpretaci√≥n sem√°ntica  

Garantiza consistencia durante la inferencia.


### 3.5 Modelo de Predicci√≥n ‚Äì Inferencia

#### üìå **Predict Endpoint Specification**

| Par√°metro | Definici√≥n |
|------------|--------------|
| **Operation** | Model Inference (`predict_proba`) |
| **Protocol** | REST |
| **Method** | POST |
| **Resource Path** | `/api/predictproba` |
| **Output Contract** | `PredictResponse` |
| **Successful Response** | 200 |
| **Error Response** | 404 |


#### 3.5.1 Modelo Entrada de Predicci√≥n ‚Äì Inferencia

**‚úÖ JSON Request API-Friendly ‚Äì /predictproba**

```json
{
  "model_id": "xgb_model_20260224_222532",
  "features": [
    {
      "client_id": "0001",
      "edad": 24,
      "ingresos_mensuales": 1500,
      "antiguedad_meses": 8,
      "incidentes_previos": 2,
      "ratio_deuda_ingresos": 0.62,
      "num_productos": 1,
      "canal": "web"
    },
    {
      "client_id": "0002",
      "edad": 47,
      "ingresos_mensuales": 3400,
      "antiguedad_meses": 72,
      "incidentes_previos": 0,
      "ratio_deuda_ingresos": 0.21,
      "num_productos": 4,
      "canal": "portal"
    },
    {
      "client_id": "0003",
      "edad": 35,
      "ingresos_mensuales": 2200,
      "antiguedad_meses": 30,
      "incidentes_previos": 1,
      "ratio_deuda_ingresos": 0.41,
      "num_productos": 3,
      "canal": "fisico"
    }
  ]
}
```

#### ‚úÖ **Explicaci√≥n JSON de Entrada ‚Äì Proceso de Inferencia (`predict_proba`)**

El objeto de entrada define el contrato de datos requerido por la API REST para ejecutar el proceso de inferencia del modelo de Machine Learning.

Este formato sigue un dise√±o **API-Friendly**, permitiendo desacoplar la representaci√≥n externa del modelo interno.

#### üîπ **model_id**

Identificador √∫nico del modelo previamente entrenado y persistido.

Permite:

‚úÖ Seleccionar din√°micamente el artefacto ML  
‚úÖ Gestionar versionado del modelo  
‚úÖ Garantizar trazabilidad de inferencia  

Durante la ejecuci√≥n, el sistema:

‚úî Recupera el pipeline serializado  
‚úî Carga el modelo en memoria  


#### üîπ **features**

Contiene la lista de registros (clientes) que ser√°n evaluados por el modelo.

Cada elemento representa una observaci√≥n independiente.

**Estructura conceptual:**

`features ‚Üí (n_samples √ó n_features)`

Donde:

- **n_samples ‚Üí** N√∫mero de clientes evaluados  
- **n_features ‚Üí** Variables predictoras del modelo  

#### üîπ **client_id**

Identificador del registro procesado.

Su funci√≥n es estrictamente operativa:

‚úî No participa en el modelo  
‚úî Permite trazabilidad y auditor√≠a  
‚úî Vincula input ‚Üî output  

#### üîπ **Variables Predictoras**

Las variables incluidas corresponden exactamente al contrato esperado por el pipeline ML:

‚úî edad  
‚úî ingresos_mensuales  
‚úî antiguedad_meses  
‚úî incidentes_previos  
‚úî ratio_deuda_ingresos  
‚úî num_productos  
‚úî canal  

Estas variables definen:

‚úÖ Dimensionalidad del modelo  
‚úÖ Orden estructural del pipeline  
‚úÖ Consistencia de inferencia  

#### 3.5.2 Modelo Salida de Predicci√≥n ‚Äì Inferencia

**‚úÖ JSON Response API-Friendly ‚Äì /predictproba**

```json
{
  "predictions": [
    {
      "client_id": "0001",
      "age": 24,
      "risk": "medium",
      "score": "93.23%",
      "risk_ranking": [
        { "risk": "high", "score": "4.16%" },
        { "risk": "low", "score": "2.61%" }
      ]
    },
    {
      "client_id": "0002",
      "age": 47,
      "risk": "low",
      "score": "87.79%",
      "risk_ranking": [
        { "risk": "medium", "score": "11.53%" },
        { "risk": "high", "score": "0.67%" }
      ]
    },
    {
      "client_id": "0003",
      "age": 35,
      "risk": "low",
      "score": "94.68%",
      "risk_ranking": [
        { "risk": "medium", "score": "4.73%" },
        { "risk": "high", "score": "0.59%" }
      ]
    }
  ]
}
```

#### **Explicaci√≥n del Predict Response ‚Äì Inferencia del Modelo**

El objeto de respuesta representa el resultado del proceso de inferencia ejecutado por el pipeline de Machine Learning.
Cada elemento dentro de `predictions[]` corresponde a un registro evaluado por el modelo.

#### üîπ **predictions**

Contiene la lista de clientes procesados por el modelo.

Cada predicci√≥n encapsula:

‚úî Identificaci√≥n del registro  
‚úî Clase predicha  
‚úî Confianza del modelo  
‚úî Distribuci√≥n probabil√≠stica completa  

#### üîπ **client_id**

Identificador del registro evaluado.

Permite:

‚úÖ Trazabilidad  
‚úÖ Auditor√≠a  
‚úÖ Integraci√≥n con sistemas externos  


#### üîπ **risk**

Clase predicha por el modelo.

Representa la categor√≠a con mayor probabilidad estimada:

- `low`
- `medium`
- `high`

#### üîπ **score**

Probabilidad asociada a la clase predicha.

Indica el nivel de confianza del modelo en su decisi√≥n.

Ejemplo:
```:
‚úî 93.23% ‚Üí Alta certeza en la predicci√≥n
```

#### üîπ **risk_ranking**

Distribuci√≥n probabil√≠stica completa del modelo.

Representa las probabilidades de las clases alternativas:

‚úî Permite interpretabilidad <br>
‚úî Permite an√°lisis de incertidumbre <br>
‚úî Soporta decisiones basadas en riesgo <br>

Ejemplo:
```:
- risk = medium
- score = 93.23%
```

### 3.6 Machine Learning System Architecture ‚Äì Blueprint 

El diagrama describe la arquitectura del arquetipo Python utilizado en la PoC, mostrando la interacci√≥n entre las capas de API (FastAPI), procesamiento de Machine Learning (Scikit-learn + XGBoost) y la gesti√≥n centralizada de excepciones.

![Texto alternativo](/dev-xgboost-learning-py/resources/docs/img/ml_system_arch.png)


| Capa | Clase / Elemento | Tipo / Patr√≥n | Responsabilidad (definici√≥n) |
|------|-------------------|---------------|-------------------------------|
| API/Controller | `/api/training` | Router/Controller (FastAPI endpoint) | Expone el endpoint de entrenamiento: recibe el request, valida el contrato, delega en `TrainingServiceImpl` y devuelve la respuesta serializada. |
| API/Controller | `/api/predictproba` | Router/Controller (FastAPI endpoint) | Expone el endpoint de inferencia/probabilidad: transforma request ‚Üí servicio, invoca `PredictServiceImpl`, devuelve probabilidades/score. |
| API/Controller | `/api/repository` | Router/Controller (FastAPI endpoint) | Expone operaciones de repositorio (listar/cargar/guardar modelos): delega en `RepositoryServiceImpl`. |
| Schemas | `TrainSchema` | DTO / Request-Response Schema (Pydantic) | Contrato de entrada/salida para training (features/target/params) y validaci√≥n de tipos. |
| Schemas | `PredictSchema` | DTO / Request-Response Schema (Pydantic) | Contrato de entrada/salida para predicci√≥n (features/model_id, etc.) y validaci√≥n de payload. |
| Schemas | `RepositorySchema` | DTO / Request-Response Schema (Pydantic) | Contrato para operaciones de repositorio (model_id, filtros, metadatos, etc.). |
| Service | `TrainingServiceImpl` | Service Layer / Orchestrator | Orquesta el flujo de entrenamiento: preparaci√≥n ‚Üí llamada a `Training` (ML layer) ‚Üí construcci√≥n de respuesta (Builders/Mappers) ‚Üí manejo de errores. |
| Service | `PredictServiceImpl` | Service Layer / Orchestrator | Orquesta el flujo de predicci√≥n: carga pipeline/modelo ‚Üí `PredictProba` ‚Üí mapea resultado a response y controla excepciones. |
| Service | `RepositoryServiceImpl` | Service Layer / Facade | Orquesta casos de uso de persistencia: invoca `Repository` (ML layer) para guardar/listar/cargar modelos y retorna DTOs amigables. |
| Service | `Builders` | Builder | Construye objetos de respuesta (Response DTOs) a partir de resultados internos (m√©tricas, predicciones, metadatos). |
| Service | `Mappers` | Mapper/Assembler | Convierte entre representaciones: Schema/API ‚Üî DTO interno ‚Üî estructuras ML (X/y) ‚Üî Response. |
| ML (Scikit/XGB) | `Training` | Use Case / ML Component | Implementa el entrenamiento: arma pipeline, ajusta modelo XGBoost, ejecuta fit, eval√∫a (si aplica) y produce artefacto entrenado + m√©tricas. |
| ML (Scikit/XGB) | `PredictProba` | Use Case / ML Component | Ejecuta inferencia con `predict_proba()` (score/probabilidades) usando pipeline entrenado. |
| ML (Scikit/XGB) | `Repository` | Repository (Persistencia ML) | Persistencia/carga del pipeline/modelo (normalmente con Joblib): versionado por `model_id`, listar modelos, recuperar artefactos. |
| ML (Scikit/XGB) | `build_preprocessor` | Factory / Builder (Pipeline) | Crea el preprocesador (ColumnTransformer, encoders, scalers) y prepara el pipeline de scikit-learn. |
| ML (Scikit/XGB) | `TrainingDto` | DTO interno | Estructura interna para transportar datos/resultado de training (model_id, m√©tricas, features, etc.) entre capas. |
| ML (Scikit/XGB) | `MetricsDto` | DTO interno | Representa m√©tricas calculadas (accuracy/precision/recall/f1, etc.) y las hace independientes del framework. |
| Exception Handler | `register_exception_handlers` | Global Exception Registration | Registra handlers globales en FastAPI para mapear excepciones ‚Üí HTTP status + body est√°ndar. |
| Exception Handler | `ServiceError` | Base Exception | Error base/est√°ndar de la capa de servicio (mensaje, c√≥digo, status, clave i18n, etc.). |
| Exception Handler | `TrainingError` | Domain/UseCase Exception | Error controlado para fallos en entrenamiento (fit, pipeline, validaci√≥n, etc.). |
| Exception Handler | `PredictProbaError` | Domain/UseCase Exception | Error controlado para fallos en inferencia/probabilidad (modelo no cargado, shape mismatch, etc.). |
| Exception Handler | `RepositoryError` | Domain/UseCase Exception | Error controlado para fallos de persistencia (save/load/list, permisos, path, artefacto corrupto). |
| Logging | `LOGGING` | Cross-cutting concern | Observabilidad transversal: logs por capa (API/Service/ML), trazabilidad de errores, m√©tricas de ejecuci√≥n, debugging. |

### 3.7 Gradient Boosting ML Pipeline Architecture 

Este dise√±o representa un enfoque transaccional de Machine Learning, en el que cada capa del sistema asume responsabilidades claramente definidas, garantizando desacoplamiento, mantenibilidad y claridad estructural dentro del contexto t√©cnico de ML

![Texto alternativo](/dev-xgboost-learning-py/resources/docs/img/arch_model.png)

La siguiente tabla resume el flujo transaccional completo del sistema, desde la interacci√≥n API-REST hasta el entrenamiento, la inferencia y la persistencia del modelo de Machine Learning.

| Fase | Capa | Componente | Operaci√≥n | Descripci√≥n Transaccional |
|------|------|-------------|-------------|-----------------------------|
| 1 | API-REST (FastAPI) | Endpoint Layer | Recepci√≥n Request JSON | La API recibe un payload JSON validado mediante modelos Pydantic, garantizando contrato y tipos de datos. |
| 2 | API-REST (FastAPI) | Transform Model | Validaci√≥n & Parsing | El request es validado y convertido en objetos tipados (DTOs), evitando errores estructurales o de tipado. |
| 3 | Service Layer | Service Facade | Orquestaci√≥n Caso de Uso | La capa de servicio act√∫a como fachada, desacoplando la API del n√∫cleo ML y dirigiendo el flujo hacia training, prediction o repository. |
| 4 | Service Layer | Transform | Adaptaci√≥n de Datos | Los datos se transforman desde la representaci√≥n API hacia estructuras consumibles por el pipeline ML (X / matrices / features). |
| 5 | Machine Learning Layer | Dataset Splitting | train_test_split | Se particiona el dataset en conjuntos de entrenamiento y validaci√≥n para evitar sesgo y permitir evaluaci√≥n objetiva. |
| 6 | Machine Learning Layer | Pipeline Orchestration | Preprocessing Pipeline | Se aplican transformaciones (scaling, encoding, feature engineering) mediante Scikit-learn. |
| 7 | Machine Learning Layer | Model Fitting | Fit (XGBoost) | El modelo XGBoost es entrenado utilizando los datos transformados, optimizando la funci√≥n objetivo. |
| 8 | Machine Learning Layer | Evaluation Phase | predict(X) & metrics | Se eval√∫a la calidad predictiva del modelo mediante m√©tricas (accuracy u otras). |
| 9 | Machine Learning Layer | Predict Phase | predict_proba(X) | Durante inferencia, el pipeline ejecuta scoring probabil√≠stico para estimaci√≥n de riesgo/fraude. |
| 10 | Machine Learning Layer | Storage Phase | Persist Model (.pkl) | El pipeline/modelo entrenado es serializado y almacenado para reutilizaci√≥n futura. |
| 11 | Machine Learning Layer | Storage Phase | Load Model (.pkl) | En predicci√≥n, el modelo persistido es cargado din√°micamente. |
| 12 | Service Layer | Builder / Mapping Object | Construcci√≥n Response | Se construyen respuestas API-friendly a partir de outputs internos (scores, m√©tricas, metadatos). |
| 13 | API-REST (FastAPI) | Response | Serializaci√≥n JSON | La respuesta es serializada a JSON garantizando contrato REST consistente. |
| 14 | Cross-Cutting | Exception Handling | Gesti√≥n de Errores | Las excepciones son interceptadas y normalizadas en errores HTTP controlados. |
| 15 | Cross-Cutting | Logging | Observabilidad | Se registran eventos operacionales, errores y m√©tricas de ejecuci√≥n. |

## 4. Decisiones Arquitect√≥nicas

---

### 1Ô∏è‚É£ ¬øPor qu√© utilizar algoritmos basados en √°rboles de decisi√≥n?

Existen varias razones fundamentales para utilizar modelos basados en √°rboles:

1. **Capturan relaciones no lineales de forma natural**  
2. **Manejan eficientemente variables heterog√©neas**, tanto num√©ricas como categ√≥ricas  
3. **No requieren normalizaci√≥n ni escalado previo**  
4. **Ofrecen excelente rendimiento en datos tabulares**, especialmente en dominios como seguros, banca o fraude  

Este tipo de algoritmos resulta especialmente adecuado para escenarios de scoring y clasificaci√≥n de riesgo.

---

### 2Ô∏è‚É£ ¬øPor qu√© utilizar Pandas para transformar modelos Pydantic en DataFrames?

El uso de Pandas introduce ventajas estructurales y operacionales:

‚úÖ Conservaci√≥n expl√≠cita de *feature names*  
‚úÖ Eliminaci√≥n de errores por orden de columnas  
‚úÖ Mayor trazabilidad y facilidad de debugging  
‚úÖ Compatibilidad directa con Scikit-learn  
‚úÖ Integraci√≥n eficiente con XGBoost  

Esta decisi√≥n mejora significativamente la robustez del pipeline.

---

### **2Ô∏è3Ô∏è‚É£ ¬øPor qu√© utilizar Joblib para la persistencia del modelo?**

Joblib es una librer√≠a de Python dise√±ada para la serializaci√≥n eficiente de objetos complejos, siendo especialmente adecuada para artefactos de Machine Learning.

Su adopci√≥n permite:

‚úÖ Persistir modelos entrenados  
‚úÖ Recuperar pipelines completos de inferencia  
‚úÖ Minimizar el coste de reconstrucci√≥n del modelo  

```python
joblib.dump(pipeline, "model.pkl")
```
---
### **4Ô∏è‚É£ ¬øPor qu√© utilizar Scikit-learn junto con XGBoost?**

La arquitectura definida para la PoC establece como criterio la implementaci√≥n del algoritmo XGBoost a trav√©s de la API compatible con Scikit-learn, con el objetivo de aprovechar su ecosistema de herramientas, manteniendo XGBoost como motor de Machine Learning.

Esta combinaci√≥n permite una clara separaci√≥n de responsabilidades:

**XGBoost ‚Üí Motor de Machine Learning**

‚úî Entrenamiento del modelo  
‚úî Predicci√≥n  
‚úî Optimizaci√≥n interna  
‚úî Gesti√≥n de boosting / √°rboles  

**Scikit-learn ‚Üí Ecosistema / Framework**

‚úî Pipelines (`Pipeline`)  
‚úî Validaci√≥n (`train_test_split`, `cross_val_score`)  
‚úî Tuning (`GridSearchCV`, `RandomizedSearchCV`)  
‚úî M√©tricas (`accuracy_score`, `roc_auc_score`)  
‚úî Preprocesamiento (`StandardScaler`, `OneHotEncoder`)  

Este enfoque maximiza la modularidad, reproducibilidad y mantenibilidad del sistema.

---
### **5Ô∏è‚É£ ¬øPor qu√© utilizar `sklearn.pipeline.Pipeline`?**

El uso de `Pipeline` permite definir un flujo secuencial, determinista y reproducible de procesamiento de datos y entrenamiento del modelo.

```python
pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", model)
])
```
El bloque **preprocessor** representa la capa de transformaci√≥n que prepara los datos antes de que el modelo aprenda o prediga.

---

### **6Ô∏è‚É£ ¬øPor qu√© persistir el pipeline completo y no solo el modelo XGBoost?**

Porque el pipeline encapsula todo el conocimiento aprendido durante el entrenamiento, no solo el modelo final.

El pipeline contiene:

‚úî Modelo entrenado  
‚úî Encoders entrenados (`OneHotEncoder`)  
‚úî Imputadores entrenados (`SimpleImputer`)  
‚úî Estad√≠sticas aprendidas  
‚úî Categor√≠as detectadas  
‚úî Reglas de transformaci√≥n 
